{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on a General Video\n",
    "The trained model is used to test the mean heart rate in every 10-second-window of a video file in its first 15 seconds.\n",
    "### Pre-setting\n",
    "- Import relevant modules\n",
    "- Define preconfigured global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "file = './general_video.avi'\n",
    "FPS = 61\n",
    "WINDOW_LEN = FPS * 10 # frame number for 10-second video clips\n",
    "time_len = 15 * FPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the ROI sequence from frames of the general video\n",
    "- detect frontal face bounding box\n",
    "- Align the bounding box using the predicted facial landmarks\n",
    "- Crop the bounding box and obtain test data: (time_len, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to extract roi sequence for file: ./general_video.avi\n",
      "The roi sequence for file  ./general_video.avi  has been extracted\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "\n",
    "def extract_bd_box_from_video(video):\n",
    "    vc = cv2.VideoCapture(video)\n",
    "    rois = []\n",
    "    idx = 0\n",
    "    rval, frame = vc.read()\n",
    "\n",
    "    while rval and idx < time_len:\n",
    "        detector = dlib.get_frontal_face_detector()\n",
    "        faces = detector(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY), 0)\n",
    "\n",
    "        if len(faces) == 0 or len(faces) > 1:\n",
    "            print(\"{0} faces are detected\".format(len(faces)))\n",
    "            rect = last_rect\n",
    "        else:\n",
    "            '''Narrow down the bounding box'''\n",
    "            rect = faces[0]\n",
    "        last_rect = rect\n",
    "\n",
    "        '''Align the bounding box using the predicted facial landmarks'''\n",
    "        predictor = dlib.shape_predictor(\"dataset/shape_predictor_68_face_landmarks.dat\")\n",
    "        bd_box = face_utils.FaceAligner(predictor, desiredFaceWidth=256).align(frame, cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY), rect) # bd_box.shape = (256, 256, 3)\n",
    "\n",
    "        '''Crop the bounding box'''\n",
    "        h, w, c = bd_box.shape\n",
    "        h_percent = 0.7\n",
    "        w_percent = 0.7\n",
    "        cut_x = int(w * (1 - w_percent) * 0.5)\n",
    "        cut_y = int(h * (1 - h_percent) * 0.5)\n",
    "        bd_box = bd_box[cut_y: h - cut_y, cut_x: w - cut_x]\n",
    "        if idx % 100 == 0:\n",
    "            cv2.imwrite(\"../results/boxes/\" + str(idx) + '.jpg', bd_box) # save the image locally\n",
    "\n",
    "        rois.append(bd_box.tolist())\n",
    "        rval, frame = vc.read()\n",
    "        idx += 1\n",
    "    return rois\n",
    "\n",
    "print(\"Start to extract roi sequence for file:\", file)\n",
    "sequence = extract_bd_box_from_video(file)\n",
    "print(\"The roi sequence for file \", file, \" has been extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the testing dataset \n",
    "- Pre-process the data of extracted ROI sequence and obtain samples:   \n",
    "resize -> bandpath filter -> apply sliding window strategy to construct samples in shape (sample_num, window_len, c=3, w=36, h=36)\n",
    "- Construct the testing dataset:   \n",
    "    **Frame input (x_appearance)**: the original content of the resized ROI sequence, which is used in the attention mechanism.  \n",
    "    **difference input (x_motion)**: the time-domain discrete derivative of the resized ROI sequence, which is the mainsource of pulse information in the 3D-CNN network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 3, 609, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "from scipy.signal import filtfilt\n",
    "from pre_processing.utils import build_bandpass_filter\n",
    "\n",
    "order = 128\n",
    "b = build_bandpass_filter(FPS, order, False)\n",
    "\n",
    "sequence = np.transpose(sequence, (0, 3, 1, 2))\n",
    "avg_pool = nn.AdaptiveAvgPool2d((36, 36))\n",
    "sequence = avg_pool(torch.from_numpy(sequence).float())\n",
    "_, _, w, h = sequence.size()\n",
    "for i in range(w):\n",
    "    for j in range(h):\n",
    "        for c in range(3):\n",
    "            sequence[:, c, i, j] = torch.from_numpy(filtfilt(b, np.array([1]), sequence[:, c, i, j], axis=0).copy())\n",
    "\n",
    "sequence = sequence.tolist()\n",
    "total_length = len(sequence)\n",
    "x_appearance = np.array([sequence[i: i + WINDOW_LEN] \n",
    "                         for i in range(0, total_length - WINDOW_LEN, FPS)])\n",
    "x_motion = []\n",
    "for window in x_appearance:\n",
    "    t = window[1] - window[0]\n",
    "    diff = [window[i] - window[i-1] for i in range(1, len(window))]\n",
    "    x_motion.append(diff)\n",
    "x_appearance = np.transpose(x_appearance[:, :-1, :, :, :], (0, 2, 1, 3, 4))\n",
    "x_motion = np.transpose(np.array(x_motion), (0, 2, 1, 3, 4))\n",
    "# print(x_appearance.shape, x_motion.shape)\n",
    "x_tensor = torch.from_numpy(np.stack((x_motion, x_appearance), axis=1)).float()\n",
    "print(x_tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing\n",
    "- Predict the mean heart rate values in every 10-second-window using the trained model on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 3, 609, 36, 36])\n",
      "[0.49523571133613586, 0.4952322542667389, 0.495260626077652, 0.49523743987083435, 0.4952356517314911]\n",
      "*********** Save the result of detection for file ./general_video.avi\n"
     ]
    }
   ],
   "source": [
    "print(x_tensor.size())\n",
    "\n",
    "def test_accuracy(model, x_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions_list = []\n",
    "#         targets_list = []\n",
    "        for sequence in x_tensor:\n",
    "            sequence = sequence.unsqueeze(0)\n",
    "#             print(sequence.size())\n",
    "            predictions = model(sequence)\n",
    "            predictions = predictions.numpy().reshape(-1)\n",
    "            predictions_list.extend(predictions.tolist())\n",
    "#         draw_picture(predictions_list, targets_list)\n",
    "        print(predictions_list)\n",
    "    return predictions_list\n",
    "#         print(targets_list)\n",
    "\n",
    "from models.cnn3d import CNN\n",
    "model = CNN()\n",
    "model.load_state_dict(torch.load('./models/best_model.pt')[0])\n",
    "model.eval()\n",
    "\n",
    "predictions_list = test_accuracy(model, x_tensor)\n",
    "with open('./result_of_detection.json', 'w') as outfile:\n",
    "    print(\"*********** Save the result of detection for file {0}\".format(file))\n",
    "    json.dump(predictions_list, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
